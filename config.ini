[web]
## The IP address and port that are used for hosting the web app.
# Default: 127.0.0.1
host = 127.0.0.1
# Default: 5000
port = 5000

[ingestion]
# Options: local, azure_blob_storage, aws_s3
method = local
# Language of the data to be ingested. Should coincide with the values of 'language' in the .json files.
language = en
# If method is local, please specify the path to the folder in the local storage containing the data
data_folder = ./data
# If method is azure_blob_storage, please specify the blob SAS url to the container containing the data
azure_container_sas_url =
# If method is aws_s3, please specify the following parameters
# Access key id of the user
aws_access_key_id =
# Secret access key of the user
aws_secret_access_key =
# Name of the bucket in the S3 storage to connect to
aws_bucket_name =
# Region name of the bucket
aws_region_name = 

[indexing]
## Embedding model to use for the vectorization
# Options: text-embedding-3-small, text-embedding-3-small, ada v2, HuggingFaceEmbeddings
embeddings = text-embedding-3-small

## Splitter used for chunking
# Options: RecursiveCharacterTextSplitter, SemanticTextSplitter
textsplitter = RecursiveCharacterTextSplitter
## Maximum chunk size for recursive character splitter
# Default: 1000
textsplitter_recursive_chunk_size = 2000
## breakpoint type for semantic text splitter
# Options: percentile, standard_deviation, interquartile
# Default: percentile
textsplitter_semantic_breakpoint_type=percentile

## Built multi-representation indexing database that uses summaries as parent documents.
## A chatbot is used for summaries
# Options: True, False
use_summaries = False
# options: openai, ollama
provider = openai
# openai: gpt-3.5-turbo | ollama: llama3, phi3, mistral
model = gpt-3.5-turbo

# Option to persist the current vectordb or use a persist directory
# Options: True, False
persist_current_vectordb = False
use_persist_directory = False
# persist_directory_path must be set to a specific directory path depending on the use case
persist_directory = persist_directories/vectordb

[retrieval]
# options: openai, ollama
provider = openai
# openai: gpt-3.5-turbo | ollama: llama3, phi3, mistral
model = gpt-3.5-turbo
# options: default
prompt = default
temperature = 0.7
# Options: Nearest Neighbor, Contextual Compression, Parent Document, SVM, Multi-Query, Ensemble
method = Nearest Neighbor
k_chunks = 5
# Options: 0 = use no reranker, 1 = use Long Context Reordering, 2 = use Cohere reranking, 3 = use Cross Encoder reranking
reranker = 0

[chatbot]
## OpenAI api key used fro all openai chatbots
openai_api_key =
## Ollama url used for all ollama chatbots
# Example: http://172.16.254.1:11434/api/generate
ollama_url = 

[generation]
# options: openai, ollama
provider = openai
# openai: gpt-3.5-turbo | ollama: llama3, phi3, mistral
model = gpt-3.5-turbo
# options: default
prompt = default
temperature = 0.7

[routing]
# options: openai, ollama
provider = openai
# openai: gpt-3.5-turbo | ollama: llama3, phi3, mistral
model = gpt-3.5-turbo
# options: default
prompt = default
temperature = 0.7

[guardrails]
# options: openai, ollama
provider = openai
# openai: gpt-3.5-turbo | ollama: llama3, phi3, mistral
model = gpt-3.5-turbo
temperature = 0.7
block_pii = True
block_not_work_related = False
anonymize_pii = False
# options: llm, presidio
anonymization_method = llm
# if anonymization_method=llm then the  deanonymization method has to be llm as well
# llm, combined_exact_fuzzy
deanonymization_method = llm

[logging]
filename=pipeline_log.log